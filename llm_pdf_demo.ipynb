{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os \n",
    "\n",
    "from config import config\n",
    "\n",
    "CHAT_PDF_API = config.CHAT_PDF_API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    ('file', ('file', open('/Users/nickaristidou/Downloads/beazley_623.pdf', 'rb'), 'application/octet-stream'))\n",
    "]\n",
    "headers = {\n",
    "    'x-api-key': f'{CHAT_PDF_API}'\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    'https://api.chatpdf.com/v1/sources/add-file', headers=headers, files=files)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('Source ID:', response.json()['sourceId'])\n",
    "    source_id = response.json()['sourceId']\n",
    "else:\n",
    "    print('Status:', response.status_code)\n",
    "    print('Error:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What gross premiums written did syndicate 263 report in 2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'x-api-key': f'{CHAT_PDF_API}',\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"referenceSources\": True,\n",
    "    'sourceId': f\"{source_id}\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': \"user\",\n",
    "            'content': f\"{question}\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    'https://api.chatpdf.com/v1/chats/message', headers=headers, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('Result:', response.json()['content'])\n",
    "    print('References:', response.json()['references'])\n",
    "else:\n",
    "    print('Status:', response.status_code)\n",
    "    print('Error:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "  'x-api-key': f'{CHAT_PDF_API}',\n",
    "  'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "data = {\n",
    "  'sources': [f'{source_id}'],\n",
    "}\n",
    "\n",
    "try:\n",
    "  response = requests.post(\n",
    "    'https://api.chatpdf.com/v1/sources/delete', json=data, headers=headers)\n",
    "  response.raise_for_status()\n",
    "  print('Success - pdf deleted')\n",
    "except requests.exceptions.RequestException as error:\n",
    "  print('Error:', error)\n",
    "  print('Response:', error.response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Version ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### curl -O https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PDFPlumberLoader, PyPDFLoader, DirectoryLoader, PyMuPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter, CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory Loader Version ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"./data\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickaristidou/.pyenv/versions/3.10.8/envs/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, embeddings, persist_directory=\"chroma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_llm = HuggingFaceHub(\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "    model_kwargs={\n",
    "        \"device_map\": \"auto\",\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"max_length\": 512,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ./ggml-gpt4all-j-v1.3-groovy.bin\n",
      "gptj_model_load: loading model from './ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 5401.45 MB\n",
      "gptj_model_load: kv self size  =  896.00 MB\n",
      "gptj_model_load: ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[89890]: Class GGMLMetalClass is implemented in both /Users/nickaristidou/.pyenv/versions/3.10.8/envs/venv/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x2c11f8208) and /Users/nickaristidou/.pyenv/versions/3.10.8/envs/venv/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x2c1444208). One of the two will be used. Which one is undefined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................ done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./ggml-gpt4all-j-v1.3-groovy.bin\"\n",
    "gpt_llm = GPT4All(model=model_path,max_tokens=510, n_batch=8, backend=\"gptj\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "        You are an artificial intelligence assistant who understands insurance.\n",
    "        Use the following pieces of context to answer the question at the end.\n",
    "        Extract numbers where appropriate to help answer questions.\n",
    "        If you don't know the answer, just say you don't know. DO NOT try to make up an answer.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        Helpful answer in markdown:\n",
    "        \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt_llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "match MODEL_NAME:\n",
    "    case \"falcon_llm\":\n",
    "        llm = falcon_llm\n",
    "    case \"gpt_llm\":\n",
    "        llm = gpt_llm\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    # chain_type_kwargs={\n",
    "    #     \"prompt\": prompt\n",
    "    # },\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the folling to see the default prompt -- run the above with no chain type kwargs\n",
    "# print(qa.combine_documents_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask questions ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 283 ms, total: 1min 10s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = qa(\n",
    "    \"What is the combined ratio for syndicate 2623 at year end 2022\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The combined ratio of Syndicate 2623 was 91% in 2022.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"source_documents\"][0].metadata[\"source\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single PDF loader -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"./data/hiscox.pdf\"\n",
    "pdf_file2 = \"/Users/nickaristidou/Downloads/alex_test.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = convert_from_path(pdf_file, dpi=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=pdf_file)\n",
    "documents = loader.load_and_split()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader2 = PyPDFLoader(file_path=pdf_file2)\n",
    "documents2 = loader2.load_and_split()\n",
    "len(documents2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(documents2[5].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter = TokenTextSplitter(\n",
    "                    chunk_size=100, chunk_overlap=10\n",
    "                )  # This the encoding for text-embedding-ada-002\n",
    "texts = token_splitter.split_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2 = text_splitter.split_documents(documents2)\n",
    "len(texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_total = texts + texts2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts_total, embeddings, persist_directory=\"chroma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n_ctx = 1000\n",
    "model_path = \"./ggml-gpt4all-j-v1.3-groovy.bin\"\n",
    "llm = GPT4All(model=model_path, backend=\"gptj\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = config.HF_API\n",
    "\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "#     model_kwargs={\n",
    "#         \"temperature\":1\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res = qa(\n",
    "    \"What combined ratio did Syndicate 33 report in 2022\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref in range(0, len(res[\"source_documents\"])):\n",
    "    print(res[\"source_documents\"][ref].metadata[\"page\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask question about 2nd pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = qa(\n",
    "    \"Give me a summary of transaction reporting for mifid? extract the response from the text.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = qa(\n",
    "    \"From page 5 of the hiscox report what was the Gross premiums written in 2022? extract the reponse from the text.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Add prompt and investigate LangchainDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as dir:\n",
    "        dir\n",
    "        shutil.copyfileobj(\"./docs/hiscox.pdf\", )\n",
    "        tmp_path = Path(tmp.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"./docs/\"+\"hiscox.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
